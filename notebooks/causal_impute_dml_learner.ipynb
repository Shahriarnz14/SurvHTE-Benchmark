{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from tqdm import tqdm\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_scenario_data(h5_file_path, scenario_num):\n",
    "    key = f\"scenario_{scenario_num}/data\"\n",
    "    with pd.HDFStore(h5_file_path, mode='r') as store:\n",
    "        if key not in store:\n",
    "            return None  # Scenario not found\n",
    "        df = store[key]\n",
    "        metadata = store.get_storer(key).attrs.metadata\n",
    "    return {\"dataset\": df, \"metadata\": metadata}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "store_files = [\n",
    "    \"../synthetic_data/RCT_0_5.h5\",\n",
    "    \"../synthetic_data/RCT_0_05.h5\",\n",
    "    \"../synthetic_data/e_X.h5\",\n",
    "    \"../synthetic_data/e_X_U.h5\",\n",
    "    \"../synthetic_data/e_X_no_overlap.h5\",\n",
    "    \"../synthetic_data/e_X_info_censor.h5\",\n",
    "    \"../synthetic_data/e_X_U_info_censor.h5\",\n",
    "    \"../synthetic_data/e_X_no_overlap_info_censor.h5\"\n",
    "]\n",
    "\n",
    "experiment_setups = {}\n",
    "\n",
    "for path in store_files:\n",
    "    base_name = os.path.splitext(os.path.basename(path))[0]  # e.g. RCT_0_5\n",
    "    scenario_dict = {}\n",
    "    # for scenario in range(1, 11):\n",
    "    for scenario in ['A', 'B', 'C', 'D', 'E']:\n",
    "        try:\n",
    "            result = load_scenario_data(path, scenario)\n",
    "            if result is not None:\n",
    "                scenario_dict[f\"scenario_{scenario}\"] = result\n",
    "        except Exception as e:\n",
    "            # Log or ignore as needed\n",
    "            continue\n",
    "    experiment_setups[base_name] = scenario_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>observed_time</th>\n",
       "      <th>event</th>\n",
       "      <th>W</th>\n",
       "      <th>X1</th>\n",
       "      <th>X2</th>\n",
       "      <th>X3</th>\n",
       "      <th>X4</th>\n",
       "      <th>X5</th>\n",
       "      <th>U1</th>\n",
       "      <th>U2</th>\n",
       "      <th>T0</th>\n",
       "      <th>T1</th>\n",
       "      <th>T</th>\n",
       "      <th>C</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0.054267</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.135488</td>\n",
       "      <td>0.887852</td>\n",
       "      <td>0.932606</td>\n",
       "      <td>0.445568</td>\n",
       "      <td>0.388236</td>\n",
       "      <td>0.151609</td>\n",
       "      <td>0.205535</td>\n",
       "      <td>0.054267</td>\n",
       "      <td>0.061394</td>\n",
       "      <td>0.054267</td>\n",
       "      <td>1.803019</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0.732630</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.257596</td>\n",
       "      <td>0.657368</td>\n",
       "      <td>0.492617</td>\n",
       "      <td>0.964238</td>\n",
       "      <td>0.800984</td>\n",
       "      <td>0.597208</td>\n",
       "      <td>0.255785</td>\n",
       "      <td>0.228566</td>\n",
       "      <td>0.732630</td>\n",
       "      <td>0.732630</td>\n",
       "      <td>1.689546</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>0.162856</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.455205</td>\n",
       "      <td>0.801058</td>\n",
       "      <td>0.041718</td>\n",
       "      <td>0.769458</td>\n",
       "      <td>0.003171</td>\n",
       "      <td>0.370382</td>\n",
       "      <td>0.223214</td>\n",
       "      <td>0.176016</td>\n",
       "      <td>0.162856</td>\n",
       "      <td>0.162856</td>\n",
       "      <td>1.256329</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>0.050340</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.292809</td>\n",
       "      <td>0.610914</td>\n",
       "      <td>0.913027</td>\n",
       "      <td>0.300115</td>\n",
       "      <td>0.248599</td>\n",
       "      <td>0.038464</td>\n",
       "      <td>0.409829</td>\n",
       "      <td>0.381909</td>\n",
       "      <td>0.050340</td>\n",
       "      <td>0.050340</td>\n",
       "      <td>1.241777</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>0.524607</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.666392</td>\n",
       "      <td>0.987533</td>\n",
       "      <td>0.468270</td>\n",
       "      <td>0.123287</td>\n",
       "      <td>0.916031</td>\n",
       "      <td>0.342961</td>\n",
       "      <td>0.791330</td>\n",
       "      <td>0.524607</td>\n",
       "      <td>1.121968</td>\n",
       "      <td>0.524607</td>\n",
       "      <td>1.516613</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id  observed_time  event  W        X1        X2        X3        X4  \\\n",
       "0   0       0.054267      1  0  0.135488  0.887852  0.932606  0.445568   \n",
       "1   1       0.732630      1  1  0.257596  0.657368  0.492617  0.964238   \n",
       "2   2       0.162856      1  1  0.455205  0.801058  0.041718  0.769458   \n",
       "3   3       0.050340      1  1  0.292809  0.610914  0.913027  0.300115   \n",
       "4   4       0.524607      1  0  0.666392  0.987533  0.468270  0.123287   \n",
       "\n",
       "         X5        U1        U2        T0        T1         T         C  \n",
       "0  0.388236  0.151609  0.205535  0.054267  0.061394  0.054267  1.803019  \n",
       "1  0.800984  0.597208  0.255785  0.228566  0.732630  0.732630  1.689546  \n",
       "2  0.003171  0.370382  0.223214  0.176016  0.162856  0.162856  1.256329  \n",
       "3  0.248599  0.038464  0.409829  0.381909  0.050340  0.050340  1.241777  \n",
       "4  0.916031  0.342961  0.791330  0.524607  1.121968  0.524607  1.516613  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "experiment_setups['RCT_0_5']['scenario_B']['dataset'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>random_idx0</th>\n",
       "      <th>random_idx1</th>\n",
       "      <th>random_idx2</th>\n",
       "      <th>random_idx3</th>\n",
       "      <th>random_idx4</th>\n",
       "      <th>random_idx5</th>\n",
       "      <th>random_idx6</th>\n",
       "      <th>random_idx7</th>\n",
       "      <th>random_idx8</th>\n",
       "      <th>random_idx9</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>idx</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>47390</td>\n",
       "      <td>5618</td>\n",
       "      <td>14210</td>\n",
       "      <td>46970</td>\n",
       "      <td>4203</td>\n",
       "      <td>16369</td>\n",
       "      <td>24535</td>\n",
       "      <td>45204</td>\n",
       "      <td>45725</td>\n",
       "      <td>45885</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>38566</td>\n",
       "      <td>46218</td>\n",
       "      <td>39045</td>\n",
       "      <td>7253</td>\n",
       "      <td>22759</td>\n",
       "      <td>34401</td>\n",
       "      <td>28889</td>\n",
       "      <td>38471</td>\n",
       "      <td>45822</td>\n",
       "      <td>37471</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>32814</td>\n",
       "      <td>20226</td>\n",
       "      <td>40012</td>\n",
       "      <td>4854</td>\n",
       "      <td>27351</td>\n",
       "      <td>39165</td>\n",
       "      <td>25359</td>\n",
       "      <td>14516</td>\n",
       "      <td>25717</td>\n",
       "      <td>29860</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>41393</td>\n",
       "      <td>39492</td>\n",
       "      <td>27153</td>\n",
       "      <td>19041</td>\n",
       "      <td>33009</td>\n",
       "      <td>19822</td>\n",
       "      <td>21243</td>\n",
       "      <td>41228</td>\n",
       "      <td>955</td>\n",
       "      <td>23901</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>12564</td>\n",
       "      <td>17823</td>\n",
       "      <td>48976</td>\n",
       "      <td>18458</td>\n",
       "      <td>22756</td>\n",
       "      <td>28169</td>\n",
       "      <td>45851</td>\n",
       "      <td>36620</td>\n",
       "      <td>29824</td>\n",
       "      <td>12711</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49995</th>\n",
       "      <td>15948</td>\n",
       "      <td>39245</td>\n",
       "      <td>30779</td>\n",
       "      <td>48178</td>\n",
       "      <td>45056</td>\n",
       "      <td>4892</td>\n",
       "      <td>528</td>\n",
       "      <td>7486</td>\n",
       "      <td>31042</td>\n",
       "      <td>38267</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49996</th>\n",
       "      <td>11102</td>\n",
       "      <td>29624</td>\n",
       "      <td>40779</td>\n",
       "      <td>3136</td>\n",
       "      <td>45904</td>\n",
       "      <td>41903</td>\n",
       "      <td>45682</td>\n",
       "      <td>36621</td>\n",
       "      <td>33204</td>\n",
       "      <td>38070</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49997</th>\n",
       "      <td>16338</td>\n",
       "      <td>8986</td>\n",
       "      <td>19293</td>\n",
       "      <td>35651</td>\n",
       "      <td>10172</td>\n",
       "      <td>17947</td>\n",
       "      <td>38843</td>\n",
       "      <td>18310</td>\n",
       "      <td>2765</td>\n",
       "      <td>12581</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49998</th>\n",
       "      <td>32478</td>\n",
       "      <td>32134</td>\n",
       "      <td>11955</td>\n",
       "      <td>36939</td>\n",
       "      <td>33266</td>\n",
       "      <td>41932</td>\n",
       "      <td>43910</td>\n",
       "      <td>21691</td>\n",
       "      <td>40801</td>\n",
       "      <td>33527</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49999</th>\n",
       "      <td>23102</td>\n",
       "      <td>2305</td>\n",
       "      <td>33545</td>\n",
       "      <td>20070</td>\n",
       "      <td>20726</td>\n",
       "      <td>1875</td>\n",
       "      <td>26988</td>\n",
       "      <td>24790</td>\n",
       "      <td>17795</td>\n",
       "      <td>39733</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>50000 rows × 10 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       random_idx0  random_idx1  random_idx2  random_idx3  random_idx4  \\\n",
       "idx                                                                      \n",
       "0            47390         5618        14210        46970         4203   \n",
       "1            38566        46218        39045         7253        22759   \n",
       "2            32814        20226        40012         4854        27351   \n",
       "3            41393        39492        27153        19041        33009   \n",
       "4            12564        17823        48976        18458        22756   \n",
       "...            ...          ...          ...          ...          ...   \n",
       "49995        15948        39245        30779        48178        45056   \n",
       "49996        11102        29624        40779         3136        45904   \n",
       "49997        16338         8986        19293        35651        10172   \n",
       "49998        32478        32134        11955        36939        33266   \n",
       "49999        23102         2305        33545        20070        20726   \n",
       "\n",
       "       random_idx5  random_idx6  random_idx7  random_idx8  random_idx9  \n",
       "idx                                                                     \n",
       "0            16369        24535        45204        45725        45885  \n",
       "1            34401        28889        38471        45822        37471  \n",
       "2            39165        25359        14516        25717        29860  \n",
       "3            19822        21243        41228          955        23901  \n",
       "4            28169        45851        36620        29824        12711  \n",
       "...            ...          ...          ...          ...          ...  \n",
       "49995         4892          528         7486        31042        38267  \n",
       "49996        41903        45682        36621        33204        38070  \n",
       "49997        17947        38843        18310         2765        12581  \n",
       "49998        41932        43910        21691        40801        33527  \n",
       "49999         1875        26988        24790        17795        39733  \n",
       "\n",
       "[50000 rows x 10 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "experiment_repeat_setups = pd.read_csv(\"../synthetic_data/idx_split.csv\").set_index(\"idx\")\n",
    "experiment_repeat_setups"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EXPERIMENT CONSTANTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_repeats_to_include = 3  # max 10\n",
    "num_training_data_points = 500 # max 45000\n",
    "test_size = 5000\n",
    "imputation_method = 'Pseudo_obs' # 'Pseudo_obs', 'Margin', 'IPCW-T'\n",
    "dml_learner_type = 'double_ml'\n",
    "load_imputed_values = True\n",
    "imputed_times_path = f\"../synthetic_data/imputed_times_lookup.pkl\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_pickle_path = f\"results/{dml_learner_type}_{imputation_method}_num_repeats_{num_repeats_to_include}_train_size_{num_training_data_points}.pkl\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run Experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "# Add the parent directory of \"notebooks\" to the Python path\n",
    "sys.path.append(os.path.abspath(os.path.join(os.getcwd(), \"..\")))\n",
    "\n",
    "from models_causal_impute.dml_learners import DoubleML, CausalForest\n",
    "from models_causal_impute.survival_eval_impute import SurvivalEvalImputer\n",
    "import time\n",
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_data_split(dataset_df, experiment_repeat_setups, random_idx_col_list, num_training_data_points=5000, test_size=5000):\n",
    "    split_results = {}\n",
    "\n",
    "    for rand_idx in random_idx_col_list:\n",
    "        random_idx = experiment_repeat_setups[rand_idx].values\n",
    "        test_ids = random_idx[-test_size:]\n",
    "        train_ids = random_idx[:min(num_training_data_points, len(random_idx) - test_size)]\n",
    "\n",
    "        X_cols = [c for c in dataset_df.columns if c.startswith(\"X\") and c[1:].isdigit()]\n",
    "        \n",
    "        train_df = dataset_df[dataset_df['id'].isin(train_ids)]\n",
    "        test_df = dataset_df[dataset_df['id'].isin(test_ids)]\n",
    "\n",
    "        X_train = train_df[X_cols].to_numpy()\n",
    "        W_train = train_df[\"W\"].to_numpy()\n",
    "        Y_train = train_df[[\"observed_time\", \"event\"]].to_numpy()\n",
    "\n",
    "        X_test = test_df[X_cols].to_numpy()\n",
    "        W_test = test_df[\"W\"].to_numpy()\n",
    "        Y_test = test_df[[\"observed_time\", \"event\"]].to_numpy()\n",
    "\n",
    "        cate_test_true = (test_df[\"T1\"] - test_df[\"T0\"]).to_numpy()\n",
    "\n",
    "        split_results[rand_idx] = (X_train, W_train, Y_train, X_test, W_test, Y_test, cate_test_true)\n",
    "\n",
    "    return split_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output results path: results/double_machine_learning_Pseudo_obs_num_repeats_3_train_size_500.pkl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "RCT_0_5 Scenarios:   0%|          | 0/5 [00:03<?, ?it/s]\n",
      "Experiment Setups:   0%|          | 0/8 [00:03<?, ?it/s]\n"
     ]
    }
   ],
   "source": [
    "random_idx_col_list = experiment_repeat_setups.columns.to_list()[:num_repeats_to_include]\n",
    "\n",
    "print(\"Output results path:\", output_pickle_path)\n",
    "\n",
    "results_dict = {}\n",
    "\n",
    "for setup_name, setup_dict in tqdm(experiment_setups.items(), desc=\"Experiment Setups\"):\n",
    "\n",
    "    results_dict[setup_name] = {}\n",
    "\n",
    "    for scenario_key in tqdm(setup_dict, desc=f\"{setup_name} Scenarios\"):\n",
    "        dataset_df = setup_dict[scenario_key][\"dataset\"]\n",
    "        split_dict = prepare_data_split(dataset_df, experiment_repeat_setups, random_idx_col_list, num_training_data_points, test_size)\n",
    "\n",
    "        # Initialize results dictionary for this setup and scenario\n",
    "        results_dict[setup_name][scenario_key] = {}\n",
    "        \n",
    "        start_time = time.time()\n",
    "\n",
    "        for rand_idx in random_idx_col_list:\n",
    "            X_train, W_train, Y_train, X_test, W_test, Y_test, cate_test_true = split_dict[rand_idx]\n",
    "\n",
    "            Y_train_imputed, Y_test_imputed = None, None\n",
    "            if load_imputed_values:\n",
    "                # Load imputed values\n",
    "                with open(imputed_times_path, \"rb\") as f:\n",
    "                    imputed_times = pickle.load(f)\n",
    "\n",
    "                # Get imputed values for the current random index\n",
    "                imputed_results = imputed_times.get(imputation_method, {}).get(setup_name, {}).get(scenario_key, {}).get(num_training_data_points, {}).get(rand_idx, {})\n",
    "                Y_train_imputed = imputed_results.get(\"Y_train_imputed\", None)\n",
    "                Y_test_imputed = imputed_results.get(\"Y_test_imputed\", None)\n",
    "            \n",
    "            # If imputed values are not loaded, use the original Y_train and Y_test\n",
    "            if Y_train_imputed is None:\n",
    "                # impute the missing values\n",
    "                print(f\"[Train] Imputing missing values for {setup_name} - {scenario_key} - {rand_idx}\")\n",
    "                survival_imputer = SurvivalEvalImputer(imputation_method=imputation_method, verbose=False)\n",
    "                Y_train_imputed, Y_test_imputed = survival_imputer.fit_transform(Y_train, Y_test)\n",
    "            \n",
    "            if Y_test_imputed is None:\n",
    "                # impute the missing values\n",
    "                print(f\"[Test] Imputing missing values for {setup_name} - {scenario_key} - {rand_idx}\")\n",
    "                survival_imputer = SurvivalEvalImputer(imputation_method=imputation_method, verbose=False)\n",
    "                _, Y_test_imputed = survival_imputer.fit_transform(Y_train, Y_test, impute_train=False)\n",
    "            \n",
    "            if dml_learner_type == 'causal_forest':\n",
    "                learner = CausalForest()\n",
    "            elif dml_learner_type == 'double_ml':\n",
    "                learner = DoubleML()\n",
    "            else:\n",
    "                raise ValueError(f\"Unknown dml-learner type: {dml_learner_type}\")\n",
    "            \n",
    "            learner.fit(X_train, W_train, Y_train_imputed)\n",
    "            mse_test, cate_test_pred, ate_test_pred = learner.evaluate(X_test, cate_test_true, W_test)\n",
    "\n",
    "            # Save results\n",
    "            results_dict[setup_name][scenario_key][rand_idx] = {\n",
    "                \"cate_true\": cate_test_true,\n",
    "                \"cate_pred\": cate_test_pred,\n",
    "                \"ate_true\": cate_test_true.mean(),\n",
    "                \"ate_pred\": ate_test_pred,\n",
    "                \"cate_mse\": mse_test,\n",
    "                \"ate_bias\": ate_test_pred - cate_test_true.mean(),\n",
    "            }\n",
    "\n",
    "        end_time = time.time()\n",
    "\n",
    "        # Save results to the setup dictionary\n",
    "        avg = results_dict[setup_name][scenario_key]\n",
    "        results_dict[setup_name][scenario_key][\"average\"] = {\n",
    "                \"mean_cate_mse\": np.mean([avg[i][\"cate_mse\"] for i in random_idx_col_list]),\n",
    "                \"std_cate_mse\": np.std([avg[i][\"cate_mse\"] for i in random_idx_col_list]),\n",
    "                \"mean_ate_pred\": np.mean([avg[i][\"ate_pred\"] for i in random_idx_col_list]),\n",
    "                \"std_ate_pred\": np.std([avg[i][\"ate_pred\"] for i in random_idx_col_list]),\n",
    "                \"mean_ate_true\": np.mean([avg[i][\"ate_true\"] for i in random_idx_col_list]),\n",
    "                \"std_ate_true\": np.std([avg[i][\"ate_true\"] for i in random_idx_col_list]),\n",
    "                \"mean_ate_bias\": np.mean([avg[i][\"ate_bias\"] for i in random_idx_col_list]),\n",
    "                \"std_ate_bias\": np.std([avg[i][\"ate_bias\"] for i in random_idx_col_list]),\n",
    "                \"runtime\": (end_time - start_time) / len(random_idx_col_list),\n",
    "            }\n",
    "\n",
    "        # Save progress to disk\n",
    "        # with open(output_pickle_path, \"wb\") as f:\n",
    "            # pickle.dump(results_dict, f)\n",
    "        \n",
    "        break\n",
    "    break\n",
    "            \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'RCT_0_5': {'scenario_1': {'random_idx0': {'cate_true': array([ 0.01964673,  0.67548137,  0.01256466, ..., -0.08345062,\n",
       "            0.20826006, -0.22013693]),\n",
       "    'cate_pred': array([0.35880349, 0.01793386, 0.04956911, ..., 0.12553503, 0.26289244,\n",
       "           0.32821794]),\n",
       "    'ate_true': 0.12915102003284332,\n",
       "    'ate_pred': 0.191805094802926,\n",
       "    'cate_mse': 0.7013657900456458,\n",
       "    'ate_bias': 0.0626540747700827},\n",
       "   'random_idx1': {'cate_true': array([ 0.04373459, -0.01348651, -0.12558549, ...,  2.88443699,\n",
       "           -0.03312028,  0.24602149]),\n",
       "    'cate_pred': array([ 0.09488152, -0.04375936,  0.19755696, ...,  0.10941645,\n",
       "           -0.00424927, -0.04086473]),\n",
       "    'ate_true': 0.12449698781077848,\n",
       "    'ate_pred': 0.03621087299856414,\n",
       "    'cate_mse': 0.732344691436326,\n",
       "    'ate_bias': -0.08828611481221434},\n",
       "   'random_idx2': {'cate_true': array([-0.13083455, -0.25443191,  0.22322109, ..., -0.04042443,\n",
       "            0.68333127, -0.12309126]),\n",
       "    'cate_pred': array([ 0.01897321,  0.11184102,  0.08931063, ..., -0.00990849,\n",
       "            0.33058642,  0.09623308]),\n",
       "    'ate_true': 0.11320535412052017,\n",
       "    'ate_pred': 0.15275459377929765,\n",
       "    'cate_mse': 0.5884304516778116,\n",
       "    'ate_bias': 0.03954923965877748},\n",
       "   'average': {'mean_cate_mse': 0.6740469777199278,\n",
       "    'std_cate_mse': 0.06184693596012466,\n",
       "    'mean_ate_pred': 0.12692352052692926,\n",
       "    'std_ate_pred': 0.06609500093932044,\n",
       "    'mean_ate_true': 0.12228445398804733,\n",
       "    'std_ate_true': 0.006695149726202448,\n",
       "    'mean_ate_bias': 0.004639066538881946,\n",
       "    'std_ate_bias': 0.06638160060453877,\n",
       "    'runtime': 1.2902618249257405}}}}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def summarize_experiment_results(results_dict):\n",
    "    records = []\n",
    "\n",
    "    for setup_name, setup_dict in results_dict.items():\n",
    "        for scenario_key in setup_dict:\n",
    "            row = {\n",
    "                (\"setup_name\", \"\"): setup_name,\n",
    "                (\"scenario_key\", \"\"): scenario_key\n",
    "            }\n",
    "\n",
    "            avg_result = setup_dict[scenario_key].get(\"average\", {})\n",
    "            mean_mse = avg_result.get(\"mean_cate_mse\", np.nan)\n",
    "            std_mse = avg_result.get(\"std_cate_mse\", np.nan)\n",
    "            mean_ate_pred = avg_result.get(\"mean_ate_pred\", np.nan)\n",
    "            std_ate_pred = avg_result.get(\"std_ate_pred\", np.nan)\n",
    "            mean_ate_true = avg_result.get(\"mean_ate_true\", np.nan)\n",
    "            std_ate_true = avg_result.get(\"std_ate_true\", np.nan)\n",
    "            mean_ate_bias = avg_result.get(\"mean_ate_bias\", np.nan)\n",
    "            std_ate_bias = avg_result.get(\"std_ate_bias\", np.nan)\n",
    "            runtime = avg_result.get(\"runtime\", np.nan)\n",
    "\n",
    "            row[(\"CATE_MSE\",\"\")] = f\"{mean_mse:.3f} ± {std_mse:.3f}\" if not pd.isna(mean_mse) else np.nan\n",
    "            row[(\"ATE_pred\",\"\")] = f\"{mean_ate_pred:.3f} ± {std_ate_pred:.3f}\" if not pd.isna(mean_ate_pred) else np.nan\n",
    "            row[(\"ATE_true\",\"\")] = f\"{mean_ate_true:.3f} ± {std_ate_true:.3f}\" if not pd.isna(mean_ate_true) else np.nan\n",
    "            row[(\"ATE_bias\",\"\")] = f\"{mean_ate_bias:.3f} ± {std_ate_bias:.3f}\" if not pd.isna(mean_ate_bias) else np.nan\n",
    "            row[(\"runtime [s]\",\"\")] = round(runtime) if not pd.isna(runtime) else np.nan\n",
    "\n",
    "            records.append(row)\n",
    "\n",
    "    df = pd.DataFrame.from_records(records)\n",
    "    df.columns = pd.MultiIndex.from_tuples(df.columns)\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "T-Learner Results\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr th {\n",
       "        text-align: left;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th>setup_name</th>\n",
       "      <th>scenario_key</th>\n",
       "      <th>CATE_MSE</th>\n",
       "      <th>ATE_pred</th>\n",
       "      <th>ATE_true</th>\n",
       "      <th>ATE_bias</th>\n",
       "      <th>runtime [s]</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>RCT_0_5</td>\n",
       "      <td>scenario_1</td>\n",
       "      <td>0.674 ± 0.062</td>\n",
       "      <td>0.127 ± 0.066</td>\n",
       "      <td>0.122 ± 0.007</td>\n",
       "      <td>0.005 ± 0.066</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  setup_name scenario_key       CATE_MSE       ATE_pred       ATE_true  \\\n",
       "                                                                         \n",
       "0    RCT_0_5   scenario_1  0.674 ± 0.062  0.127 ± 0.066  0.122 ± 0.007   \n",
       "\n",
       "        ATE_bias runtime [s]  \n",
       "                              \n",
       "0  0.005 ± 0.066           1  "
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"T-Learner Results\")\n",
    "summary_df = summarize_experiment_results(results_dict)\n",
    "summary_df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "causal_survival_db",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
